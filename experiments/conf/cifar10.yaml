seed: 42

trainer:
    devices: -1
    accelerator: gpu
    max_epochs: -1
    accumulate_grad_batches: 1
    strategy: auto
    precision: bf16-mixed

datamodule:
    _target_: tinyedm.datamodule.CIFAR10DataModule
    batch_size: 128
    num_workers: 16
    image_size: 32
    data_dir: datasets/cifar10


model:
    _target_: tinyedm.EDM
    diffuser:
        _target_: tinyedm.Diffuser
        P_std: 1.2
        P_mean: -1.2

    embedding:
        _target_: tinyedm.Embedding
        fourier_dim: 64
        embedding_dim: 256
        num_classes: 10

    denoiser:
        _target_: tinyedm.Denoiser
        in_channels: 3
        out_channels: 3
        sigma_data: 0.5
        embedding_dim: ${model.embedding.embedding_dim}
        encoder_block_types: ["Enc", "Enc", "Enc", "EncD", "EncA", "EncA", "EncA", "EncD", "EncA", "EncA", "EncA"]
        decoder_block_types: ["DecA", "Dec", "DecA", "DecA", "DecA", "DecA", "DecU", "DecA", "DecA", "DecA", "DecA", "DecU", "Dec", "Dec", "Dec", "Dec"]
        encoder_out_channels: [128, 128, 128, 128, 256, 256, 256, 256, 512, 512, 512]
        decoder_out_channels: [512, 512, 512, 512, 512, 512, 512, 256, 256, 256, 256, 256, 128, 128, 128, 128]
        skip_connections: [False, False, True, True, True, True, False, True, True, True, True, False, True, True, True, True]

    use_uncertainty: False
    lr: 0.01
    steady_steps: 500
    rampup_steps: 500

    use_ema: False
    ema_length: 0.1
    validate_original_weights: False
    cpu_offload: False
    every_n_steps: 1
    

solver:
    _target_: tinyedm.DeterministicSolver
    num_steps: 32
    sigma_min: 0.002
    sigma_max: 80.0
    rho: 7.0
    dtype: float32

# callbacks
checkpoint_callback:
    monitor: train_loss 
    mode: min
    save_top_k: 1
    verbose: True

generate_callback:
    num_samples: 8
    img_shape: [3, 32, 32]
    every_n_epochs: 5

wandb:
    project: CIFAR10
    resume: True
    reinit: False

wandb_watch:
    log_freq: 500
    log: all
